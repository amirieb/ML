<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Syllabus and Textbooks</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">COMP4220</div>
<div class="menu-item"><a href=".">Home</a></div>
<div class="menu-item"><a href="syllabus.html" class="current">Syllabus</a></div>
<div class="menu-item"><a href="grading.html">Grading</a></div>
<div class="menu-item"><a href="admins.html">Admins</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Syllabus and Textbooks</h1>
</div>
<p>The readings will be from select chapters of the following <b>freely-available</b> textbooks and lecture notes:</p>
<ul>
<li><p><b><a target='_new' href="https://probml.github.io/pml-book/book2.html">Probabilistic Machine Learning</a></b> by Kevin P. Murphy. MIT Press, 2023.</p>
</li>
<li><p><b><a target='_new' href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of Machine Learning</a></b> by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. MIT Press, Second Edition, 2018.</p>
</li>
<li><p><b><a target='_new' href="https://www.bishopbook.com/">Deep Learning: Foundations and Concepts</a></b> by Christopher Bishop and Hugh Bishop. Springer Nature, 2023.</p>
</li>
</ul>
<h2>Syllabus</h2>
<p><b>W1. Introduction to ML</b></p>
<ul>
<li><p>Machine Learning definition, types, and applications</p>
</li>
<li><p>K-Nearest Neighbors (KNNs)</p>
</li>
<li><p>Course administration</p>
</li>
<li><p><b>Example</b>:  KNN example</p>
</li>
</ul>
<p><b>W2. Naïve Bayes and Logistic Regression</b></p>
<ul>
<li><p>Probabilistic classification</p>
</li>
<li><p>Estimating probability distributions</p>
</li>
<li><p>Contrasting Naïve Bayes (NB) and Logistic Regression</p>
</li>
<li><p><b>Example</b>:  NB example</p>
</li>
<li><p><b>Due</b>:  KNN</p>
</li>
</ul>
<p><b>W3. Stochastic Gradient Descent (SGD)</b> </p>
<ul>
<li><p>Convexity and optimization</p>
</li>
<li><p>Step size selection</p>
</li>
<li><p>Regularized conditional log-likelihood</p>
</li>
<li><p><b>Example</b>:  SGD for Logistic Regression example </p>
</li>
</ul>
<p><b>W4. Data Engineering and PAC learnability</b></p>
<ul>
<li><p>Consistent hypotheses and finite spaces</p>
</li>
<li><p>Connection to Interval Learning</p>
</li>
<li><p><b>Example</b>:  Californian in Boston</p>
</li>
<li><p><b>Due</b>:  Logistic Regression I</p>
</li>
</ul>
<p><b>W5. Support Vector Machines (SVMs)</b></p>
<ul>
<li><p>Vector space models and linear classifiers</p>
</li>
<li><p>Theoretical guarantees of SVMs</p>
</li>
<li><p>Slack variables in SVMs</p>
</li>
<li><p>Kernel SVMs</p>
</li>
<li><p><b>Example</b>:  Slack example </p>
</li>
<li><p><b>Due</b>:  Logistic Regression II</p>
</li>
</ul>
<p><b>W6. Boosting</b></p>
<ul>
<li><p>Training error and weak learners</p>
</li>
<li><p>Gradient boosting</p>
</li>
<li><p><b>Example</b>:  Boosting example </p>
</li>
</ul>
<p><b>W7. Regression</b></p>
<ul>
<li><p>Linear regression</p>
</li>
<li><p>Regularization techniques</p>
</li>
<li><p><b>Example</b>:  Fitting a linear regression example </p>
</li>
<li><p><b>Example</b>:  Predicting MPG example</p>
</li>
<li><p><b>Due</b>:  SVM</p>
</li>
<li><p><b>Midterm Review</b>:  Weeks 1&ndash;8</p>
</li>
</ul>
<p><b>W8. Clustering I and Midterm</b></p>
<ul>
<li><p>k-Means</p>
</li>
<li><p><b>Example</b>:  k-Means example</p>
</li>
<li><p><b>Midterm</b>:  Assessing knowledge from Weeks 1&ndash;8</p>
</li>
</ul>
<p><b>W9. Clustering II and Proposal Presentations</b></p>
<ul>
<li><p>Gaussian Mixture Models</p>
</li>
<li><p>Limitations of k-means and GMMs</p>
</li>
<li><p><b>Project</b>:  Proposal presentation: initial feedback and guidance</p>
</li>
<li><p><b>Due</b>:  project proposal & slides</p>
</li>
</ul>
<p><b>W10. Structured Perceptron and Multilayer Networks</b></p>
<ul>
<li><p>Online learning % seperate this from structured prediciton maybe</p>
</li>
<li><p>Structured Perceptron</p>
</li>
<li><p>Activation and objective functions</p>
</li>
<li><p>Backpropigation algorithm</p>
</li>
<li><p><b>Example</b>:  POS Example</p>
</li>
<li><p><b>Example</b>:  Grid example</p>
</li>
<li><p><b>Due</b>:  Boosting </p>
</li>
</ul>
<p><b>W11. Fairness</b></p>
<ul>
<li><p>Introduction to fairness metrics and bias mitigation</p>
</li>
<li><p>Debiasing techniques</p>
</li>
<li><p><b>Example</b>:  Pneumonia example</p>
</li>
</ul>
<p><b>W12. Training Paradigms</b></p>
<ul>
<li><p>Introduction to curriculum learning</p>
</li>
<li><p>Sample difficulty and curriculum design</p>
</li>
<li><p>Applications and use cases</p>
</li>
<li><p>Performance metrics</p>
</li>
<li><p><b>Example</b>:  SVMs convergence and generalization example </p>
</li>
</ul>
<p><b>W13. Machine Unlearning</b></p>
<ul>
<li><p>Introduction to machine unlearning</p>
</li>
<li><p>Algorithmic approaches  </p>
</li>
<li><p>Applications and use cases</p>
</li>
<li><p>Performance metrics</p>
</li>
<li><p><b>Example</b>:  User account unlearning example</p>
</li>
<li><p><b>Due</b>:  Regression</p>
</li>
</ul>
<p><b>W14. Final Project Presentations and Reports</b></p>
<ul>
<li><p><b>Project</b>:  Final presentation</p>
</li>
<li><p><b>Due</b>:  Final report, project slides</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-12-25 18:35:31 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
